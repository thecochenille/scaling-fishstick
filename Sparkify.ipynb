{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import isnan, count, when, col, desc, udf, col, sort_array, asc, desc, countDistinct, first\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import to_date, year, month, dayofmonth, dayofweek, hour, date_format, substring,datediff\n",
    "from pyspark.sql.functions import sum as spark_sum, avg as spark_avg\n",
    "\n",
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify Data Exploration\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1700172386471'),\n",
       " ('spark.driver.port', '32841'),\n",
       " ('spark.driver.host',\n",
       "  '5863f353be737fc38d9029e58da34b2002b1ee70-6856f894f4-kkj8j'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'Sparkify Data Exploration'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://5863f353be737fc38d9029e58da34b2002b1ee70-6856f894f4-kkj8j:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Sparkify Data Exploration</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3b2b2a9c50>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"mini_sparkify_event_data.json\"\n",
    "user_log = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing out header\n",
    "user_log.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out the first record\n",
    "user_log.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like one row is a record of a song played, so we have the artist name and the song name, as well as the length of the session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_log.filter(col(\"page\") == \"Cancel\").take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the user is in a page without song playing, length it None, so length correspond to the length of the song played."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data and invalid data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start exploring the data using SQL, so we need to create a temporary view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------+\n",
      "|     col_name|data_type|comment|\n",
      "+-------------+---------+-------+\n",
      "|       artist|   string|   null|\n",
      "|         auth|   string|   null|\n",
      "|    firstName|   string|   null|\n",
      "|       gender|   string|   null|\n",
      "|itemInSession|   bigint|   null|\n",
      "|     lastName|   string|   null|\n",
      "|       length|   double|   null|\n",
      "|        level|   string|   null|\n",
      "|     location|   string|   null|\n",
      "|       method|   string|   null|\n",
      "|         page|   string|   null|\n",
      "| registration|   bigint|   null|\n",
      "|    sessionId|   bigint|   null|\n",
      "|         song|   string|   null|\n",
      "|       status|   bigint|   null|\n",
      "|           ts|   bigint|   null|\n",
      "|    userAgent|   string|   null|\n",
      "|       userId|   string|   null|\n",
      "+-------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log.createOrReplaceTempView(\"user_log_table\")\n",
    "spark.sql('DESC user_log_table\\\n",
    "          ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suggested let's see if userId has any missing data or sessionId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+\n",
      "|artist|auth|firstName|gender|itemInSession|lastName|length|level|location|method|page|registration|sessionId|song|status| ts|userAgent|userId|\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Missing or invalid data in userids or sessionid\n",
    "spark.sql('SELECT * \\\n",
    "           FROM user_log_table \\\n",
    "           WHERE userId IS NULL \\\n",
    "            ').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+\n",
      "|artist|auth|firstName|gender|itemInSession|lastName|length|level|location|method|page|registration|sessionId|song|status| ts|userAgent|userId|\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * \\\n",
    "           FROM user_log_table \\\n",
    "           WHERE sessionId IS NULL \\\n",
    "            ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no null values for both userId or sessionId. Maybe missing data is coded differently. Let's look at the userId list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "|      |\n",
      "|    10|\n",
      "|   100|\n",
      "|100001|\n",
      "|100002|\n",
      "|100003|\n",
      "|100004|\n",
      "|100005|\n",
      "|100006|\n",
      "|100007|\n",
      "|100008|\n",
      "|100009|\n",
      "|100010|\n",
      "|100011|\n",
      "|100012|\n",
      "|100013|\n",
      "|100014|\n",
      "|100015|\n",
      "|100016|\n",
      "|100017|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT DISTINCT userId \\\n",
    "           FROM user_log_table \\\n",
    "           ORDER BY userId \\\n",
    "            ').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|sessionId|\n",
      "+---------+\n",
      "|        1|\n",
      "|        2|\n",
      "|        3|\n",
      "|        4|\n",
      "|        5|\n",
      "|        6|\n",
      "|        7|\n",
      "|        8|\n",
      "|        9|\n",
      "|       10|\n",
      "|       11|\n",
      "|       12|\n",
      "|       13|\n",
      "|       15|\n",
      "|       16|\n",
      "|       17|\n",
      "|       18|\n",
      "|       19|\n",
      "|       20|\n",
      "|       21|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT DISTINCT sessionId \\\n",
    "           FROM user_log_table \\\n",
    "           ORDER BY sessionId \\\n",
    "            ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are userids that have empty strings. Who are these users?\n",
    "Since we don't know if this is the same user or multiple users, we are going to remove all userid with empty strings. We know that in auth, there is a guest value, let's check if the empty string userid are those who are guest so they did not register to the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|empty_strings_in_userid|\n",
      "+-----------------------+\n",
      "|                   8346|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT COUNT(userId) AS empty_strings_in_userid \\\n",
    "           FROM user_log_table \\\n",
    "           WHERE userId = \"\" \\\n",
    "            ').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      auth|\n",
      "+----------+\n",
      "|Logged Out|\n",
      "|     Guest|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT DISTINCT auth\\\n",
    "           FROM user_log_table \\\n",
    "           WHERE userId = \"\" \\\n",
    "          ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that both logged out and guests do not have a userId, so it makes sense to remove them, since we cannot associated them with any user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering out rows with '' for userId\n",
    "condition = user_log.userId != ''\n",
    "user_log_filtered= user_log.filter(condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rebuild database view\n",
    "user_log_filtered.createOrReplaceTempView(\"user_log_table_filtered\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|num_records_after_filtering|\n",
      "+---------------------------+\n",
      "|                     278154|\n",
      "+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT COUNT(*) num_records_after_filtering\\\n",
    "           FROM user_log_table_filtered\\\n",
    "           ').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|userId_count|\n",
      "+------------+\n",
      "|         225|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT COUNT(DISTINCT userId) AS userId_count\\\n",
    "           FROM user_log_table_filtered\\\n",
    "           ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 278154 remaining records and 225 users. (which makes sense since userid with empty strings counted as 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue with the user_log_filtered df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|                page|number_visits|\n",
      "+--------------------+-------------+\n",
      "|               About|          495|\n",
      "|          Add Friend|         4277|\n",
      "|     Add to Playlist|         6526|\n",
      "|              Cancel|           52|\n",
      "|Cancellation Conf...|           52|\n",
      "|           Downgrade|         2055|\n",
      "|               Error|          252|\n",
      "|                Help|         1454|\n",
      "|                Home|        10082|\n",
      "|              Logout|         3226|\n",
      "|            NextSong|       228108|\n",
      "|         Roll Advert|         3933|\n",
      "|       Save Settings|          310|\n",
      "|            Settings|         1514|\n",
      "|    Submit Downgrade|           63|\n",
      "|      Submit Upgrade|          159|\n",
      "|         Thumbs Down|         2546|\n",
      "|           Thumbs Up|        12551|\n",
      "|             Upgrade|          499|\n",
      "+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT page, COUNT(*) AS number_visits\\\n",
    "            FROM user_log_table_filtered\\\n",
    "            GROUP BY page\\\n",
    "            ORDER BY page').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|     auth|count|\n",
      "+---------+-----+\n",
      "|Cancelled|   52|\n",
      "|Logged In|  225|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking unique values in auth\n",
    "spark.sql('SELECT DISTINCT auth, COUNT (DISTINCT userId) AS count\\\n",
    "            FROM user_log_table_filtered \\\n",
    "            GROUP BY auth\\\n",
    "            ').show()\n",
    "#auth show the status of the user when they are using the platform\n",
    "#NB: check the number of unique users using the platform as cancelled and see if it matched with churn users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This look right, there are a total of 225 unique users who at some point had auth= logged in and the 52 users among these who cancelled.\n",
    "Since this will represent churn too, I will drop auth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|status|\n",
      "+------+\n",
      "|   307|\n",
      "|   404|\n",
      "|   200|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking unique values in status\n",
    "spark.sql('SELECT DISTINCT status\\\n",
    "            FROM user_log_table\\\n",
    "            ').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|method|\n",
      "+------+\n",
      "|   PUT|\n",
      "|   GET|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking unique values in method\n",
    "spark.sql('SELECT DISTINCT method\\\n",
    "            FROM user_log_table\\\n",
    "            ').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                page|\n",
      "+--------------------+\n",
      "|              Cancel|\n",
      "|    Submit Downgrade|\n",
      "|         Thumbs Down|\n",
      "|                Home|\n",
      "|           Downgrade|\n",
      "|         Roll Advert|\n",
      "|              Logout|\n",
      "|       Save Settings|\n",
      "|Cancellation Conf...|\n",
      "|               About|\n",
      "| Submit Registration|\n",
      "|            Settings|\n",
      "|               Login|\n",
      "|            Register|\n",
      "|     Add to Playlist|\n",
      "|          Add Friend|\n",
      "|            NextSong|\n",
      "|           Thumbs Up|\n",
      "|                Help|\n",
      "|             Upgrade|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT DISTINCT page\\\n",
    "          FROM user_log_table').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|number_rows|\n",
      "+-----------+\n",
      "|     286500|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#How many records does the dataset have?\n",
    "spark.sql('SELECT COUNT(*) AS number_rows\\\n",
    "            FROM user_log_table').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|unique_users|\n",
      "+------------+\n",
      "|         226|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#How many distint users?\n",
    "spark.sql('SELECT COUNT(DISTINCT userid) AS unique_users\\\n",
    "            FROM user_log_table\\\n",
    "            ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset there are 226 unique users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|paid_free|\n",
      "+---------+\n",
      "|      196|\n",
      "|      166|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how many paid and free users?\n",
    "spark.sql('SELECT COUNT(DISTINCT userid) AS paid_free\\\n",
    "            FROM user_log_table\\\n",
    "            GROUP BY level').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't sum up so it means that at some point, some users went from paid to free, or vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at page visits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The churn column will be create using Cancellation Confirmation, which is the same number as Cancel. We can quickly check if the userId who went on cancel are the same as those who went to Cancellation confirmation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature documentation and decision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data includes the following columns:\n",
    "- artist (string) : artist/singer name - keep\n",
    "- auth (string): user status - **drop as redundant with churn**\n",
    "- firstName (string) : user first name - **drop as redundant with userid**\n",
    "- gender (string): user's gender (female or male) - keep - can be changed to categorical\n",
    "- itemInSession (long): number of events during the current session - keep\n",
    "- lastName (string) : user's lastname - **drop as redundant with userid**\n",
    "- length (double): duration of song played - keep \n",
    "- level (string): subscription level (paid or free) - keep - can be changed to categorical.\n",
    "- location (string): user's location format city, state (in the USA) - keep- can be engineered by separating state\n",
    "- method (string): (put or get) - **drop**\n",
    "- page (string): page where the user is - keep\n",
    "- registration (long): timestamp of when the user registered. - keep - change time format, and check if there is any data for guests.\n",
    "- sessionId (long): unique Id for the session the user is in ***drop***\n",
    "- song (string): song playing - keep\n",
    "- status (long): looks like page error status\n",
    "- ts (long): timestamp , must be time when the user started the session \n",
    "- userAgent ( string ) : browser keep and change to categorical\n",
    "- userId ( string) unique Id per user - keep \n",
    "\n",
    "#### Features to drop\n",
    "- firstname, lastname, method, auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = (\"firstname\",\"lastname\",\"method\",\"auth\")\n",
    "user_log = user_log.drop(*cols)\n",
    "user_log.printSchema()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------+\n",
      "|     col_name|data_type|comment|\n",
      "+-------------+---------+-------+\n",
      "|       artist|   string|   null|\n",
      "|         auth|   string|   null|\n",
      "|       gender|   string|   null|\n",
      "|itemInSession|   bigint|   null|\n",
      "|       length|   double|   null|\n",
      "|        level|   string|   null|\n",
      "|     location|   string|   null|\n",
      "|         page|   string|   null|\n",
      "| registration|   bigint|   null|\n",
      "|    sessionId|   bigint|   null|\n",
      "|         song|   string|   null|\n",
      "|       status|   bigint|   null|\n",
      "|           ts|   bigint|   null|\n",
      "|    userAgent|   string|   null|\n",
      "|       userId|   string|   null|\n",
      "+-------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log.createOrReplaceTempView(\"user_log_table\")\n",
    "spark.sql('DESC user_log_table\\\n",
    "          ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "This will include:\n",
    "- taking care of any time related features: ts and registration\n",
    "- creating new features using time related features: registration month etc..\n",
    "- identifying missing data and make a decision on these\n",
    "- finalizing features selected and check their type (categorical binary etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformating time features into date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|           ts| registration|\n",
      "+-------------+-------------+\n",
      "|1538352117000|1538173362000|\n",
      "|1538352180000|1538331630000|\n",
      "|1538352394000|1538173362000|\n",
      "|1538352416000|1538331630000|\n",
      "|1538352676000|1538173362000|\n",
      "|1538352678000|1538331630000|\n",
      "|1538352886000|1538331630000|\n",
      "|1538352899000|1538173362000|\n",
      "|1538352905000|1538173362000|\n",
      "|1538353084000|1538173362000|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# these two columns have timestamps so they will be engineered to extract data on time (day, month, year)\n",
    "spark.sql('SELECT ts, registration \\\n",
    "          FROM user_log_table\\\n",
    "          LIMIT 10 ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ts and registration are timestamps so we will convert into readable time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log = user_log.withColumn('ts_ts', (col('ts') / 1000.0).cast(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log = user_log.withColumn('ts_todate', to_date('ts_ts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log = user_log.withColumn('registration_ts', (col('registration') / 1000.0).cast(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log = user_log.withColumn('registration_todate', to_date('registration_ts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = (\"ts\",\"registration\",\"ts_ts\",\"registration_ts\")\n",
    "user_log = user_log.drop(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-------+\n",
      "|           col_name|data_type|comment|\n",
      "+-------------------+---------+-------+\n",
      "|             artist|   string|   null|\n",
      "|               auth|   string|   null|\n",
      "|             gender|   string|   null|\n",
      "|      itemInSession|   bigint|   null|\n",
      "|             length|   double|   null|\n",
      "|              level|   string|   null|\n",
      "|           location|   string|   null|\n",
      "|               page|   string|   null|\n",
      "|          sessionId|   bigint|   null|\n",
      "|               song|   string|   null|\n",
      "|             status|   bigint|   null|\n",
      "|          userAgent|   string|   null|\n",
      "|             userId|   string|   null|\n",
      "|          ts_todate|     date|   null|\n",
      "|registration_todate|     date|   null|\n",
      "+-------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#recreate sql view\n",
    "user_log.createOrReplaceTempView(\"user_log_table\")\n",
    "spark.sql('DESC user_log_table').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ts and registration were reformated and are now dates. Let's take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "| ts_todate|registration_todate|\n",
      "+----------+-------------------+\n",
      "|2018-10-01|         2018-09-28|\n",
      "|2018-10-01|         2018-09-30|\n",
      "|2018-10-01|         2018-09-28|\n",
      "|2018-10-01|         2018-09-30|\n",
      "|2018-10-01|         2018-09-28|\n",
      "|2018-10-01|         2018-09-30|\n",
      "|2018-10-01|         2018-09-30|\n",
      "|2018-10-01|         2018-09-28|\n",
      "|2018-10-01|         2018-09-28|\n",
      "|2018-10-01|         2018-09-28|\n",
      "|2018-10-01|         2018-09-30|\n",
      "|2018-10-01|         2018-09-30|\n",
      "|2018-10-01|         2018-09-28|\n",
      "|2018-10-01|         2018-09-30|\n",
      "|2018-10-01|         2018-09-30|\n",
      "|2018-10-01|         2018-09-28|\n",
      "|2018-10-01|         2018-09-30|\n",
      "|2018-10-01|         2018-09-19|\n",
      "|2018-10-01|         2018-09-28|\n",
      "|2018-10-01|         2018-09-30|\n",
      "+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT ts_todate, registration_todate\\\n",
    "           FROM user_log_table').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what time interval are the records of this subset?\n",
    "spark.sql('SELECT MIN(ts_todate), MAX(ts_todate)\\\n",
    "           FROM user_log_table_filtered\\\n",
    "          ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SELECT COUNT(DISTINCT userid)\\\n",
    "           FROM user_log_table_filtered\\\n",
    "           WHERE page = \"Cancellation Confirmation\"\\\n",
    "          ').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "52/225*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the small dataset has 52 churning users which makes 23% rate on this data set. Let's check over what timeframe the dataset includes sessions.\n",
    "\n",
    "Let's use Cancellation Confirmation to create the churn label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_clean = user_log_filtered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create the churn label\n",
    "create_churn = udf(lambda x: 1 if x ==\"Cancellation Confirmation\" else 0, IntegerType())\n",
    "\n",
    "user_log_clean = user_log_clean.withColumn('churn_lab', create_churn('page'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_clean.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only defined churn binary coding based on 'page', and that the whole dataset is based on sessionid, we are going to have users that churned but with a 0 label for all the pages they have been to that are not cancellation confirmation.\n",
    "\n",
    "Now let's replace all the 0 of churn users to 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all the users with 1 for churn_lab\n",
    "churn_userid = user_log_clean.select('userId').where(col('churn_lab')==1).groupby('userId').count()\n",
    "churn_userid_list = [row['userId'] for row in churn_userid.collect()]\n",
    "len(churn_userid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_clean = user_log_clean.withColumn('churn_lab2', when((user_log_clean.userId).isin(churn_userid_list), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_clean.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_clean.createOrReplaceTempView(\"user_log_table_clean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SELECT userId, churn_lab, churn_lab2\\\n",
    "           FROM user_log_table_clean \\\n",
    "           WHERE churn_lab2 = 1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_log_clean = user_log_clean.drop(\"churn_lab\")\n",
    "user_log_clean.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for churn labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for churn labeling\n",
    "def create_churn_label(df):\n",
    "    ''' This function takes a dataframe and creates churn labels based on the \n",
    "    churn definition: Cancellation Confirmation value in 'pages' is considered churn.\n",
    "    \n",
    "    Input = Spark dataframe\n",
    "    \n",
    "    Output = Spark dataframe with new column 'churn_lab' for each user who churned\n",
    "    '''\n",
    "    #labeling all 'pages' with Cancellation Confirmation\n",
    "    create_churn = udf(lambda x: 1 if x ==\"Cancellation Confirmation\" else 0, IntegerType())\n",
    "    df = df.withColumn('churn_lab_temp', create_churn('page'))\n",
    "    \n",
    "    #extract a list of churn users from the new df\n",
    "    churn_userid = df.select('userId').where(col('churn_lab_temp')==1).groupby('userId').count()\n",
    "    churn_userid_list = [row['userId'] for row in churn_userid.collect()]\n",
    "    \n",
    "    #new column based on the churn list\n",
    "    df = df.withColumn('churn_lab', when((user_log_clean.userId).isin(churn_userid_list), 1).otherwise(0))\n",
    "    \n",
    "    #dropping column\n",
    "    df = df.drop('churn_lab_temp')\n",
    "    \n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's test the function on a copy of the dataframe\n",
    "user_log_test = user_log_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_test_churn = create_churn_label(user_log_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_test_churn.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_test_churn.select('churn_lab').groupby('churn_lab').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean data sql view\n",
    "user_log_clean.createOrReplaceTempView(\"user_log_table_clean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('DESC user_log_table_clean').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Session count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many sessions did each userId create?\n",
    "session_count_by_churn = user_log_clean.groupby('userId','churn_lab2').agg(count(\"*\").alias(\"count\")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot of session count for each user.\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.barplot(data=session_count_by_churn, x='userId', y='count', hue='churn_lab2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=\"churn_lab2\", y=\"count\", data=session_count_by_churn, palette={0: 'blue', 1: 'orange'})\n",
    "\n",
    "# Labels and Title\n",
    "plt.xlabel('Churn Label')\n",
    "plt.ylabel('Session Counts')\n",
    "plt.title('Box Plot of Session Count by Churn Label')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_by_churn = user_log_clean.groupby('artist','churn_lab2').agg(count(\"*\").alias(\"count\")).toPandas().sort_values(by=['count'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_by_churn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the most played artist is coldplay with no churn, and over 40000 sessions did not play any songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'auth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_clean.select('auth').groupby('auth').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either I remove this feature or I can explore it later with when the loggin happened."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take all users, churn and gender, drop duplicate rows by users, \n",
    "gender_by_churn = user_log_clean.dropDuplicates(['userId']).groupBy('gender','churn_lab2').agg(count('*').alias('user_count')).toPandas()\n",
    "gender_by_churn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=gender_by_churn, x='gender', y='user_count', hue='churn_lab2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_clean.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the average session length per user and churning\n",
    "columns = ['userId','length','churn_lab2']\n",
    "user_log_clean_length_df = user_log_clean.select(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_agg_df = user_log_clean_length_df.groupBy(\"userId\", \"churn_lab2\").agg(\n",
    "                spark_sum(\"length\").alias(\"total_session_length\"),\n",
    "                spark_avg(\"length\").alias(\"avg_session_length\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_agg_df.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_agg_pd = length_agg_df.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_agg_pd.sort_values(\"total_session_length\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if I have teh same userid with 2 churnlab2\n",
    "length_agg_pd.duplicated(\"userId\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_agg_pd[\"userId\"]=length_agg_pd[\"userId\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=\"churn_lab2\", y=\"total_session_length\", data=length_agg_pd, palette={0: 'blue', 1: 'orange'})\n",
    "\n",
    "plt.xlabel('Churn Label')\n",
    "plt.ylabel('Total Session Length')\n",
    "plt.title('Box Plot of Total Session Length by Churn Label')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=\"churn_lab2\", y=\"avg_session_length\", data=length_agg_pd, palette={0: 'blue', 1: 'orange'})\n",
    "\n",
    "plt.xlabel('Churn Label')\n",
    "plt.ylabel('Average Session Length')\n",
    "plt.title('Box Plot of Average Session Length by Churn Label')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if the both churned customers and not churned have the same average session length, not churn customers have a short total session length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_agg_pd[length_agg_pd['total_session_length'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_clean.dropDuplicates(['userId']).select('level').groupby('level').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this columns is related to the cancellation that I used for churn labeling so I am dropping it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_clean.dropDuplicates(['userId']).select('location').groupby('location').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split state and city?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_clean.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pages_userid = (\n",
    "    user_log_clean.groupBy(\"userId\")\n",
    "    .agg(\n",
    "        countDistinct(\"page\").alias(\"UniquePages\"),\n",
    "        first(\"churn_lab2\").alias(\"churn_lab2\")  # Assuming it's a binary label (0 or 1)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=\"churn_lab2\", y=\"UniquePages\", data=unique_pages_userid.toPandas(), palette={0: 'blue', 1: 'orange'})\n",
    "\n",
    "plt.xlabel('Churn Label')\n",
    "plt.ylabel('Average Session Length')\n",
    "plt.title('Box Plot of Unique Pages visited by Churn Label')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could count the number of unique pages the user has looked at "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_song_userid = (\n",
    "    user_log_clean.groupBy(\"userId\")\n",
    "    .agg(\n",
    "        countDistinct(\"song\").alias(\"UniqueSongs\"),\n",
    "        first(\"churn_lab2\").alias(\"churn_lab2\")  # Assuming it's a binary label (0 or 1)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=\"churn_lab2\", y=\"UniqueSongs\", data=unique_song_userid.toPandas(), palette={0: 'blue', 1: 'orange'})\n",
    "\n",
    "plt.xlabel('Churn Label')\n",
    "plt.ylabel('Number of unique songs listened to')\n",
    "plt.title('Box Plot Unique Songs by Churn Label')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SELECT status, COUNT(status) AS count\\\n",
    "           FROM user_log_table_clean\\\n",
    "           GROUP BY status').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropping this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "userAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_clean.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userAgent_userid = (\n",
    "    user_log_clean.groupBy(\"userId\")\n",
    "    .agg(first(\"userAgent\").alias(\"userAgent\"),\n",
    "        first(\"churn_lab2\").alias(\"churn_lab2\")  # Assuming it's a binary label (0 or 1)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userAgent_userid.toPandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "there is a lot of information in this column. I will extract the internet explorer platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ts_todate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_clean.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#isolate userid and dates\n",
    "columns=['userId', 'ts_todate','registration_todate','churn_lab2']\n",
    "dates_df = user_log_clean.select(columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_df= dates_df.withColumn(\"RegistrationDate\", col(\"registration_todate\").cast(\"date\"))\n",
    "dates_df = dates_df.withColumn(\"SessionDate\", col(\"ts_todate\").cast(\"date\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_df = dates_df.withColumn(\"NumDaysSinceRegistration\", datediff(col(\"SessionDate\"), col(\"RegistrationDate\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_df.select('NumDaysSinceRegistration','UserId','churn_lab2').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_df.filter(col(\"userid\") == 24).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is cancellation confirmation page the last date that a chrun user has used the sparkify?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_users_df = user_log_clean.filter(col('churn_lab')==1)\n",
    "churn_users_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churnuser_page_cancelled = churn_users_df.filter(col('page') =='Cancellation Confirmation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churnuser_page_cancelled.select([\"userId\",\"ts_todate\"]).orderBy(col(\"ts_todate\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_log_clean.select([\"userId\",\"ts_todate\"]).orderBy(col(\"ts_todate\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "registration_todate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the registration and ts timestamps, I will calculate the number of days between the registration day and the session ts.\n",
    "then I will create one col for each user with the largest number of days, the number of day where there was the largest number of sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's take a look at the schema again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- select features to keep\n",
    "- define what features need to be engineered and what to do\n",
    "-- timestamp\n",
    "\n",
    "#create udf functions using lambda\n",
    "function_name = udf(lambda x: action(, x), IntegerType())\n",
    "\n",
    "#to create new columns\n",
    "df.withColumn(\"newcolname\",function(df.oldcolname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_clean.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    '''This function takes a the spark dataframe and executes the following main steps:\n",
    "    - checking and removing missing data in userId\n",
    "    - drops all features that won't be used in the model\n",
    "    - churn labeling\n",
    "    - reformat timestamp to dates\n",
    "    - from here, aggregation by userid\n",
    "    - create new features: avg_song_length, avg_session_length, duration_registered, session_count, count_by_page_type\n",
    "    - \n",
    "    \n",
    "    Input: spark dataframe\n",
    "    Output: clean spark dataframe including one row per userId and new features'''\n",
    "    \n",
    "    #feature drop\n",
    "    'level'\n",
    "    #cols_to_drop = (\"firstname\",\"lastname\",\"method\",\"auth\",\"level\")\n",
    "    #df = df.drop(*cols)\n",
    "    \n",
    "    #missing data\n",
    "    \n",
    "    #churn labeling\n",
    "    #use function created earlier\n",
    "    \n",
    "    \n",
    "    \n",
    "    #new features and aggregation by feature\n",
    "    #allows to aggregate many features in one step\n",
    "  #  agg_df = spark_df.groupBy(\"User_ID\").agg(\n",
    "  #                            mean(\"Feature1\").alias(\"Avg_Feature1\"),\n",
    "  #                            sum(\"Feature1\").alias(\"Sum_Feature1\"),\n",
    "  #                            max(\"Feature2\").alias(\"Max_Feature2\"),\n",
    "   #                           first(\"gender\").alias(\"gender\"),\n",
    "   #                           first(\"userAgent\").alias(\"userAgent\"),\n",
    "    #                          first(\"location\").alias(\"location\"),\n",
    "    #                          spark_sum(\"length\").alias(\"total_session_length\"),\n",
    "    #                          spark_avg(\"length\").alias(\"avg_session_length\"),\n",
    "    #                          first(\"churn_lab\").alias(\"churn_label\"),\n",
    "    #                          first(\"level\").alias(\"level\"),\n",
    "    )\n",
    "\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data\n",
    "train, test = df.randomSplit(weights=[0.8,0.2], seed=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give the model accuracy\n",
    "lr.Model2.summary.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline - create a variable for each model and then use the function Pipeline\n",
    "\n",
    "\n",
    "model1 =\n",
    "model2 = \n",
    "\n",
    "pipeline = Pipeline(stages=[model,model2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plrModel = pipeline.fit(train) #trains the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = plrModel.transform(test) #predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
