{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import isnan, count, when, col, desc, udf, col, sort_array, asc, avg\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import to_date, year, month, dayofmonth, dayofweek, hour, date_format, substring\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify Data Exploration\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.host',\n",
       "  '5863f353be737fc38d9029e58da34b2002b1ee70-6fd5dfd766-7m7xw'),\n",
       " ('spark.app.id', 'local-1699391482393'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'Sparkify Data Exploration'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.port', '40569'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://5863f353be737fc38d9029e58da34b2002b1ee70-6fd5dfd766-7m7xw:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Sparkify Data Exploration</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f758ad9c940>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"mini_sparkify_event_data.json\"\n",
    "user_log = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing out header\n",
    "user_log.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out the first record\n",
    "user_log.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start exploring the data using SQL, so we need to create a temporary view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------+\n",
      "|     col_name|data_type|comment|\n",
      "+-------------+---------+-------+\n",
      "|       artist|   string|   null|\n",
      "|         auth|   string|   null|\n",
      "|    firstName|   string|   null|\n",
      "|       gender|   string|   null|\n",
      "|itemInSession|   bigint|   null|\n",
      "|     lastName|   string|   null|\n",
      "|       length|   double|   null|\n",
      "|        level|   string|   null|\n",
      "|     location|   string|   null|\n",
      "|       method|   string|   null|\n",
      "|         page|   string|   null|\n",
      "| registration|   bigint|   null|\n",
      "|    sessionId|   bigint|   null|\n",
      "|         song|   string|   null|\n",
      "|       status|   bigint|   null|\n",
      "|           ts|   bigint|   null|\n",
      "|    userAgent|   string|   null|\n",
      "|       userId|   string|   null|\n",
      "+-------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log.createOrReplaceTempView(\"user_log_table\")\n",
    "spark.sql('DESC user_log_table\\\n",
    "          ').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      auth|\n",
      "+----------+\n",
      "|Logged Out|\n",
      "| Cancelled|\n",
      "|     Guest|\n",
      "| Logged In|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking unique values in auth\n",
    "spark.sql('SELECT DISTINCT auth\\\n",
    "            FROM user_log_table\\\n",
    "            ').show()\n",
    "#auth show the status of the user when they are using the platform\n",
    "#NB: check the number of unique users using the platform as cancelled and see if it matched with churn users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|status|\n",
      "+------+\n",
      "|   307|\n",
      "|   404|\n",
      "|   200|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking unique values in status\n",
    "spark.sql('SELECT DISTINCT status\\\n",
    "            FROM user_log_table\\\n",
    "            ').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|method|\n",
      "+------+\n",
      "|   PUT|\n",
      "|   GET|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking unique values in method\n",
    "spark.sql('SELECT DISTINCT method\\\n",
    "            FROM user_log_table\\\n",
    "            ').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                page|\n",
      "+--------------------+\n",
      "|              Cancel|\n",
      "|    Submit Downgrade|\n",
      "|         Thumbs Down|\n",
      "|                Home|\n",
      "|           Downgrade|\n",
      "|         Roll Advert|\n",
      "|              Logout|\n",
      "|       Save Settings|\n",
      "|Cancellation Conf...|\n",
      "|               About|\n",
      "| Submit Registration|\n",
      "|            Settings|\n",
      "|               Login|\n",
      "|            Register|\n",
      "|     Add to Playlist|\n",
      "|          Add Friend|\n",
      "|            NextSong|\n",
      "|           Thumbs Up|\n",
      "|                Help|\n",
      "|             Upgrade|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT DISTINCT page\\\n",
    "          FROM user_log_table').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|number_rows|\n",
      "+-----------+\n",
      "|     286500|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#How many records does the dataset have?\n",
    "spark.sql('SELECT COUNT(*) AS number_rows\\\n",
    "            FROM user_log_table').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|unique_users|\n",
      "+------------+\n",
      "|         226|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#How many distint users?\n",
    "spark.sql('SELECT COUNT(DISTINCT userid) AS unique_users\\\n",
    "            FROM user_log_table\\\n",
    "            ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset there are 226 unique users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|paid_free|\n",
      "+---------+\n",
      "|      196|\n",
      "|      166|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how many paid and free users?\n",
    "spark.sql('SELECT COUNT(DISTINCT userid) AS paid_free\\\n",
    "            FROM user_log_table\\\n",
    "            GROUP BY level').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't sum up so it means that at some point, some users went from paid to free, or vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at page visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|                page|number_visits|\n",
      "+--------------------+-------------+\n",
      "|               About|          924|\n",
      "|          Add Friend|         4277|\n",
      "|     Add to Playlist|         6526|\n",
      "|              Cancel|           52|\n",
      "|Cancellation Conf...|           52|\n",
      "|           Downgrade|         2055|\n",
      "|               Error|          258|\n",
      "|                Help|         1726|\n",
      "|                Home|        14457|\n",
      "|               Login|         3241|\n",
      "|              Logout|         3226|\n",
      "|            NextSong|       228108|\n",
      "|            Register|           18|\n",
      "|         Roll Advert|         3933|\n",
      "|       Save Settings|          310|\n",
      "|            Settings|         1514|\n",
      "|    Submit Downgrade|           63|\n",
      "| Submit Registration|            5|\n",
      "|      Submit Upgrade|          159|\n",
      "|         Thumbs Down|         2546|\n",
      "+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT page, COUNT(*) AS number_visits\\\n",
    "            FROM user_log_table\\\n",
    "            GROUP BY page\\\n",
    "            ORDER BY page').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The churn column will be create using Cancellation Confirmation, which is the same number as Cancel. We can quickly check if the userId who went on cancel are the same as those who went to Cancellation confirmation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data includes the following columns:\n",
    "- artist (string) : artist/singer name - keep\n",
    "- auth (string): user status - keep and might be important \n",
    "- firstName (string) : user first name - do not keep\n",
    "- gender (string): user's gender (female or male) - keep - can be changed to categorical\n",
    "- itemInSession (long): number of events during the current session - keep\n",
    "- lastName (string) : user's lastname - do not keep\n",
    "- length (double): duration of song played - keep \n",
    "- level (string): subscription level (paid or free) - keep - can be changed to categorical.\n",
    "- location (string): user's location format city, state (in the USA) - keep- can be engineered by separating state\n",
    "- method (string): (put or get) - do not keep \n",
    "- page (string): page where the user is - keep\n",
    "- registration (long): timestamp of when the user registered. - keep - change time format, and check if there is any data for guests.\n",
    "- sessionId (long): unique Id for the session the user is in - explore before discarding\n",
    "- song (string): song playing - keep\n",
    "- status (long): looks like page error status\n",
    "- ts (long): timestamp , must be time when the user started the session \n",
    "- userAgent ( string ) : browser keep and change to categorical\n",
    "- userId ( string) unique Id per user - keep \n",
    "\n",
    "#### Features to drop\n",
    "- firstname, lastname, method, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = (\"firstname\",\"lastname\",\"method\")\n",
    "user_log = user_log.drop(*cols)\n",
    "user_log.printSchema()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------+\n",
      "|     col_name|data_type|comment|\n",
      "+-------------+---------+-------+\n",
      "|       artist|   string|   null|\n",
      "|         auth|   string|   null|\n",
      "|       gender|   string|   null|\n",
      "|itemInSession|   bigint|   null|\n",
      "|       length|   double|   null|\n",
      "|        level|   string|   null|\n",
      "|     location|   string|   null|\n",
      "|         page|   string|   null|\n",
      "| registration|   bigint|   null|\n",
      "|    sessionId|   bigint|   null|\n",
      "|         song|   string|   null|\n",
      "|       status|   bigint|   null|\n",
      "|           ts|   bigint|   null|\n",
      "|    userAgent|   string|   null|\n",
      "|       userId|   string|   null|\n",
      "+-------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log.createOrReplaceTempView(\"user_log_table\")\n",
    "spark.sql('DESC user_log_table\\\n",
    "          ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "This will include:\n",
    "- taking care of any time related features: ts and registration\n",
    "- creating new features using time related features: registration month etc..\n",
    "- identifying missing data and make a decision on these\n",
    "- finalizing features selected and check their type (categorical binary etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformating time features into date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|           ts| registration|\n",
      "+-------------+-------------+\n",
      "|1538352117000|1538173362000|\n",
      "|1538352180000|1538331630000|\n",
      "|1538352394000|1538173362000|\n",
      "|1538352416000|1538331630000|\n",
      "|1538352676000|1538173362000|\n",
      "|1538352678000|1538331630000|\n",
      "|1538352886000|1538331630000|\n",
      "|1538352899000|1538173362000|\n",
      "|1538352905000|1538173362000|\n",
      "|1538353084000|1538173362000|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# these two columns have timestamps so they will be engineered to extract data on time (day, month, year)\n",
    "spark.sql('SELECT ts, registration \\\n",
    "          FROM user_log_table\\\n",
    "          LIMIT 10 ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ts and registration are timestamps so we will convert into readable time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log = user_log.withColumn('ts_ts', (col('ts') / 1000.0).cast(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log = user_log.withColumn('ts_todate', to_date('ts_ts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log = user_log.withColumn('registration_ts', (col('registration') / 1000.0).cast(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log = user_log.withColumn('registration_todate', to_date('registration_ts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = (\"ts\",\"registration\",\"ts_ts\",\"registration_ts\")\n",
    "user_log = user_log.drop(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-------+\n",
      "|           col_name|data_type|comment|\n",
      "+-------------------+---------+-------+\n",
      "|             artist|   string|   null|\n",
      "|               auth|   string|   null|\n",
      "|             gender|   string|   null|\n",
      "|      itemInSession|   bigint|   null|\n",
      "|             length|   double|   null|\n",
      "|              level|   string|   null|\n",
      "|           location|   string|   null|\n",
      "|               page|   string|   null|\n",
      "|          sessionId|   bigint|   null|\n",
      "|               song|   string|   null|\n",
      "|             status|   bigint|   null|\n",
      "|          userAgent|   string|   null|\n",
      "|             userId|   string|   null|\n",
      "|          ts_todate|     date|   null|\n",
      "|registration_todate|     date|   null|\n",
      "+-------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#recreate sql view\n",
    "user_log.createOrReplaceTempView(\"user_log_table\")\n",
    "spark.sql('DESC user_log_table').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ts and registration were reformated and are now dates. Let's take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "| ts_todate|registration_todate|\n",
      "+----------+-------------------+\n",
      "|2018-10-01|         2018-09-28|\n",
      "|2018-10-01|         2018-09-30|\n",
      "|2018-10-01|         2018-09-28|\n",
      "|2018-10-01|         2018-09-30|\n",
      "|2018-10-01|         2018-09-28|\n",
      "|2018-10-01|         2018-09-30|\n",
      "|2018-10-01|         2018-09-30|\n",
      "|2018-10-01|         2018-09-28|\n",
      "|2018-10-01|         2018-09-28|\n",
      "|2018-10-01|         2018-09-28|\n",
      "|2018-10-01|         2018-09-30|\n",
      "|2018-10-01|         2018-09-30|\n",
      "|2018-10-01|         2018-09-28|\n",
      "|2018-10-01|         2018-09-30|\n",
      "|2018-10-01|         2018-09-30|\n",
      "|2018-10-01|         2018-09-28|\n",
      "|2018-10-01|         2018-09-30|\n",
      "|2018-10-01|         2018-09-19|\n",
      "|2018-10-01|         2018-09-28|\n",
      "|2018-10-01|         2018-09-30|\n",
      "+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT ts_todate, registration_todate\\\n",
    "           FROM user_log_table').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data and invalid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+-------------+------+-----+--------+----+---------+----+------+---------+------+---------+-------------------+\n",
      "|artist|auth|gender|itemInSession|length|level|location|page|sessionId|song|status|userAgent|userId|ts_todate|registration_todate|\n",
      "+------+----+------+-------------+------+-----+--------+----+---------+----+------+---------+------+---------+-------------------+\n",
      "+------+----+------+-------------+------+-----+--------+----+---------+----+------+---------+------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Missing or invalid data in userids or sessionid\n",
    "spark.sql('SELECT * \\\n",
    "           FROM user_log_table \\\n",
    "           WHERE userId IS NULL \\\n",
    "            ').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+-------------+------+-----+--------+----+---------+----+------+---------+------+---------+-------------------+\n",
      "|artist|auth|gender|itemInSession|length|level|location|page|sessionId|song|status|userAgent|userId|ts_todate|registration_todate|\n",
      "+------+----+------+-------------+------+-----+--------+----+---------+----+------+---------+------+---------+-------------------+\n",
      "+------+----+------+-------------+------+-----+--------+----+---------+----+------+---------+------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * \\\n",
    "           FROM user_log_table \\\n",
    "           WHERE sessionId IS NULL \\\n",
    "            ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no null values in userId or sessionId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "|      |\n",
      "|    10|\n",
      "|   100|\n",
      "|100001|\n",
      "|100002|\n",
      "|100003|\n",
      "|100004|\n",
      "|100005|\n",
      "|100006|\n",
      "|100007|\n",
      "|100008|\n",
      "|100009|\n",
      "|100010|\n",
      "|100011|\n",
      "|100012|\n",
      "|100013|\n",
      "|100014|\n",
      "|100015|\n",
      "|100016|\n",
      "|100017|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT DISTINCT userId \\\n",
    "           FROM user_log_table \\\n",
    "           ORDER BY userId \\\n",
    "            ').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|sessionId|\n",
      "+---------+\n",
      "|        1|\n",
      "|        2|\n",
      "|        3|\n",
      "|        4|\n",
      "|        5|\n",
      "|        6|\n",
      "|        7|\n",
      "|        8|\n",
      "|        9|\n",
      "|       10|\n",
      "|       11|\n",
      "|       12|\n",
      "|       13|\n",
      "|       15|\n",
      "|       16|\n",
      "|       17|\n",
      "|       18|\n",
      "|       19|\n",
      "|       20|\n",
      "|       21|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT DISTINCT sessionId \\\n",
    "           FROM user_log_table \\\n",
    "           ORDER BY sessionId \\\n",
    "            ').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|empty_strings_in_userid|\n",
      "+-----------------------+\n",
      "|                   8346|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT COUNT(userId) AS empty_strings_in_userid \\\n",
    "           FROM user_log_table \\\n",
    "           WHERE userId = \"\" \\\n",
    "            ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are userids that have empty strings. Who are these users?\n",
    "Since we don't know if this is the same user or multiple users, we are going to remove all userid with empty strings. We know that in auth, there is a guest value, let's check if the empty string userid are those who are guest so they did not register to the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      auth|\n",
      "+----------+\n",
      "|Logged Out|\n",
      "|     Guest|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT DISTINCT auth\\\n",
    "           FROM user_log_table \\\n",
    "           WHERE userId = \"\" \\\n",
    "          ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that both logged out and guests do not have a userId, so it makes sense to remove them, since we cannot associated them with any user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering out rows with '' for userId\n",
    "condition = user_log.userId != ''\n",
    "user_log_filtered = user_log.filter(condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rebuild database view\n",
    "user_log_filtered.createOrReplaceTempView(\"user_log_table_filtered\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|num_records_after_filtering|\n",
      "+---------------------------+\n",
      "|                     278154|\n",
      "+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT COUNT(*) num_records_after_filtering\\\n",
    "           FROM user_log_table_filtered\\\n",
    "           ').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|userId_count|\n",
      "+------------+\n",
      "|         225|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT COUNT(DISTINCT userId) AS userId_count\\\n",
    "           FROM user_log_table_filtered\\\n",
    "           ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 278154 remaining records and 225 users. (which makes sense since userid with empty strings counted as 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|min(ts_todate)|max(ts_todate)|\n",
      "+--------------+--------------+\n",
      "|    2018-10-01|    2018-12-03|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# what time interval are the records of this subset?\n",
    "spark.sql('SELECT MIN(ts_todate), MAX(ts_todate)\\\n",
    "           FROM user_log_table_filtered\\\n",
    "          ').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|count(DISTINCT userid)|\n",
      "+----------------------+\n",
      "|                    52|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT COUNT(DISTINCT userid)\\\n",
    "           FROM user_log_table_filtered\\\n",
    "           WHERE page = \"Cancellation Confirmation\"\\\n",
    "          ').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.11111111111111"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "52/225*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the small dataset has 52 churning users which makes 23% rate on this data set. Let's check over what timeframe the dataset includes sessions.\n",
    "\n",
    "Let's use Cancellation Confirmation to create the churn label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_clean = user_log_filtered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create the churn label\n",
    "create_churn = udf(lambda x: 1 if x ==\"Cancellation Confirmation\" else 0, IntegerType())\n",
    "\n",
    "user_log_clean = user_log_clean.withColumn('churn_lab', create_churn('page'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Martha Tilston', auth='Logged In', gender='M', itemInSession=50, length=277.89016, level='paid', location='Bakersfield, CA', page='NextSong', sessionId=29, song='Rockpools', status=200, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30', ts_todate=datetime.date(2018, 10, 1), registration_todate=datetime.date(2018, 9, 28), churn_lab=0)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log_clean.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only defined churn binary coding based on 'page', and that the whole dataset is based on sessionid, we are going to have users that churned but with a 0 label for all the pages they have been to that are not cancellation confirmation.\n",
    "\n",
    "Now let's replace all the 0 of churn users to 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect all the users with 1 for churn_lab\n",
    "churn_userid = user_log_clean.select('userId').where(col('churn_lab')==1).groupby('userId').count()\n",
    "churn_userid_list = [row['userId'] for row in churn_userid.collect()]\n",
    "len(churn_userid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_clean = user_log_clean.withColumn('churn_lab2', when((user_log_clean.userId).isin(churn_userid_list), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Martha Tilston', auth='Logged In', gender='M', itemInSession=50, length=277.89016, level='paid', location='Bakersfield, CA', page='NextSong', sessionId=29, song='Rockpools', status=200, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30', ts_todate=datetime.date(2018, 10, 1), registration_todate=datetime.date(2018, 9, 28), churn_lab=0, churn_lab2=0)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log_clean.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_clean.createOrReplaceTempView(\"user_log_table_clean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------+\n",
      "|userId|churn_lab|churn_lab2|\n",
      "+------+---------+----------+\n",
      "|    54|        0|         1|\n",
      "|    54|        0|         1|\n",
      "|    54|        0|         1|\n",
      "|    54|        0|         1|\n",
      "|    54|        0|         1|\n",
      "|    54|        0|         1|\n",
      "|    54|        0|         1|\n",
      "|    54|        0|         1|\n",
      "|   101|        0|         1|\n",
      "|   101|        0|         1|\n",
      "|   101|        0|         1|\n",
      "|   101|        0|         1|\n",
      "|   101|        0|         1|\n",
      "|   101|        0|         1|\n",
      "|   101|        0|         1|\n",
      "|   101|        0|         1|\n",
      "|   101|        0|         1|\n",
      "|   101|        0|         1|\n",
      "|   101|        0|         1|\n",
      "|   101|        0|         1|\n",
      "+------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT userId, churn_lab, churn_lab2\\\n",
    "           FROM user_log_table_clean \\\n",
    "           WHERE churn_lab2 = 1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Martha Tilston', auth='Logged In', gender='M', itemInSession=50, length=277.89016, level='paid', location='Bakersfield, CA', page='NextSong', sessionId=29, song='Rockpools', status=200, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30', ts_todate=datetime.date(2018, 10, 1), registration_todate=datetime.date(2018, 9, 28), churn_lab2=0)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "user_log_clean = user_log_clean.drop(\"churn_lab\")\n",
    "user_log_clean.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for churn labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for churn labeling\n",
    "def create_churn_label(df):\n",
    "    ''' This function takes a dataframe and creates churn labels based on the \n",
    "    churn definition: Cancellation Confirmation value in 'pages' is considered churn.\n",
    "    \n",
    "    Input = Spark dataframe\n",
    "    \n",
    "    Output = Spark dataframe with new column 'churn_lab' for each user who churned\n",
    "    '''\n",
    "    #labeling all 'pages' with Cancellation Confirmation\n",
    "    create_churn = udf(lambda x: 1 if x ==\"Cancellation Confirmation\" else 0, IntegerType())\n",
    "    df = df.withColumn('churn_lab_temp', create_churn('page'))\n",
    "    \n",
    "    #extract a list of churn users from the new df\n",
    "    churn_userid = df.select('userId').where(col('churn_lab_temp')==1).groupby('userId').count()\n",
    "    churn_userid_list = [row['userId'] for row in churn_userid.collect()]\n",
    "    \n",
    "    #new column based on the churn list\n",
    "    df = df.withColumn('churn_lab', when((user_log_clean.userId).isin(churn_userid_list), 1).otherwise(0))\n",
    "    \n",
    "    #dropping column\n",
    "    df = df.drop('churn_lab_temp')\n",
    "    \n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's test the function on a copy of the dataframe\n",
    "user_log_test = user_log_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_test_churn = create_churn_label(user_log_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Martha Tilston', auth='Logged In', gender='M', itemInSession=50, length=277.89016, level='paid', location='Bakersfield, CA', page='NextSong', sessionId=29, song='Rockpools', status=200, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30', ts_todate=datetime.date(2018, 10, 1), registration_todate=datetime.date(2018, 9, 28), churn_lab=0)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log_test_churn.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|churn_lab| count|\n",
      "+---------+------+\n",
      "|        1| 44864|\n",
      "|        0|233290|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_log_test_churn.select('churn_lab').groupby('churn_lab').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean ddata sql view\n",
    "user_log_clean.createOrReplaceTempView(\"user_log_table_clean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-------+\n",
      "|           col_name|data_type|comment|\n",
      "+-------------------+---------+-------+\n",
      "|             artist|   string|   null|\n",
      "|               auth|   string|   null|\n",
      "|             gender|   string|   null|\n",
      "|      itemInSession|   bigint|   null|\n",
      "|             length|   double|   null|\n",
      "|              level|   string|   null|\n",
      "|           location|   string|   null|\n",
      "|               page|   string|   null|\n",
      "|          sessionId|   bigint|   null|\n",
      "|               song|   string|   null|\n",
      "|             status|   bigint|   null|\n",
      "|          userAgent|   string|   null|\n",
      "|             userId|   string|   null|\n",
      "|          ts_todate|     date|   null|\n",
      "|registration_todate|     date|   null|\n",
      "|         churn_lab2|      int|   null|\n",
      "+-------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DESC user_log_table_clean').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
